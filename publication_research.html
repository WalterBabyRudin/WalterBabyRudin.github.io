<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<title>Research Statement </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
<div class="menu-category">Research and Teaching</div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="publication_research.html" class="current">Research&nbsp;Statement</a></div>
<div class="menu-item"><a href="course.html">Courseworks&nbsp;and&nbsp;Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research Statement </h1>
</div>
<p>My long-term goal is to develop a dependable decision-making strategy to tackle data uncertainty, which is a common challenge in various real-life applications.
My research proposed methodologies to study different decision-making problems with offline, noisy, small-sample, or high-dimensional data. 
On the one hand, we focus on developing computationally efficient methodologies through the lens of modern optimization techniques. 
On the other hand, we provide strong performance guarantees of the proposed modeling leveraging tools from statistics. More specifically, my research focuses on:</p>
<ul>
<li><p>Distributionally Robust Optimization</p>
</li>
<li><p>Efficient Statistical Hypothesis Testing</p>
</li>
<li><p>Reliable Multi-Hop Network Communication</p>
</li>
</ul>
<h3>Distributionally Robust Optimization</h3>
<p>Distributionally robust optimization (DRO) presents a promising approach to data-driven optimization by 
finding a minimax robust optimal decision that minimizes the expected loss under the most adverse distribution within a given set of relevant distributions. 
My research has advanced the theory, algorithms, and applications in this area.</p>
<ul>
<li><p>From a theoretical perspective, I developed a tractable strong dual reformulation for DRO utilizing the entropic-regularized Wasserstein distance, 
known as Sinkhorn DRO. 
This model improves the generalization and computational performance compared with the classical Wasserstein DRO model. </p>
</li>
<li><p>Algorithm-wise, my work introduced a first-order method that efficiently identifies near-optimal solutions with low computational and storage costs. 
It demonstrates that Sinkhorn DRO can generally be solved with a complexity comparable to that of non-robust empirical risk minimization.
As a byproduct of this analysis, we also provide sample complexity for a more general class of problems, including bilevel program and stochastic program whose 
unbiased gradient oracles are unavailable to query.</p>
</li>
<li><p>Additionally, I have explored and demonstrated the advantages of DRO across various application domains, 
including reinforcement learning, hypothesis testing, healthcare, wireless communication, and more. 
These applications highlight the versatility and effectiveness of the DRO framework in addressing diverse real-world challenges.</p>
</li>
</ul>
<table class="imgtable"><tr><td>
<img src="fig_c1.png" alt="alt text" height="260px" />&nbsp;</td>
<td align="left"></td></tr></table>
<ol>
<li><p><a href="https://arxiv.org/abs/2408.11084">Multi-level Monte-Carlo Gradient Methods for Stochastic Optimization with Biased Oracles</a><br />
Yifan Hu, <b>Jie Wang</b>, Xin Chen, Niao He.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2408.09672">Regularization for Adversarial Robust Learning</a> <a href="presentation_ISMP_pdf.pdf">[Slides]</a><br />
<b>Jie Wang</b>, Rui Gao, Yao Xie. (<font color=red <b>Winner of the 18th INFORMS DMDA Workshop Best Paper Competition Award, 2 out of 57</b></font>)</p>
</li>
<li><p><a href="https://pubsonline.informs.org/doi/10.1287/opre.2022.2382">Reliable Off-policy Evaluation for Reinforcement Learning</a><br />
<b>Jie Wang</b>, Rui Gao, Hongyuan Zha. <a href="https://pubsonline.informs.org/doi/full/10.1287/opre.2022.2382">Operations Research</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2109.11926">Sinkhorn Distributionally Robust Optimization</a> <a href="https://github.com/WalterBabyRudin/SDRO_poster/tree/main">[Poster with LaTeX Source File]</a>
<a href="https://github.com/WalterBabyRudin/SDRO_code">[Experiment Code]</a><br />
<b>Jie Wang</b>, Rui Gao, Yao Xie. Major Revision at <a href="https://pubsonline.informs.org/journal/opre">Operations Research</a><br />
(<font color=red <b*>Winner of 2022 INFORMS Best Poster Award!</b></font>)</p>
</li>
<li><p><a href="https://arxiv.org/abs/2405.08194">Distributionally Robust Degree Optimization for BATS Codes</a><br />
Hoover H. F. Yin, <b>Jie Wang</b>, Sherman S. M. Chow. <a href="https://2024.ieee-isit.org/home">2024 IEEE International Symposium on Information Theory</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2403.14822">Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets</a><br />
<b>Jie Wang</b>, Rui Gao, Yao Xie. <a href="https://2024.ieee-isit.org/home">2024 IEEE International Symposium on Information Theory</a></p>
</li>
<li><p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/f77d9409647c096789067c09455858a2-Paper-Conference.pdf">Conditional Stochastic Bilevel Optimization</a><br />
Yifan Hu, <b>Jie Wang</b>, Yao Xie, Andreas Krause, Daniel Kuhn. NeurIPS 2023 (Journal version to be submitted to <a href="https://pubsonline.informs.org/journal/opre">Operations Research</a>)</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/10476945">Reliable Adaptive Recoding for Batched Network Coding with Burst-Noise Channels</a><br />
<b>Jie Wang</b>, Talha Bozkus, Yao Xie, Urbashi Mitra. Asilomar 2023</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v193/wang22a.html">Improving Sepsis Prediction Model Generalization With Optimal Transport</a><br />
<b>Jie Wang</b>, Ronald Moore, Rishikesan Kamaleswaran, Yao Xie. 2022 Machine Learning for Health (ML4H)</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9834367">A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn Uncertainty Sets</a><br />
<b>Jie Wang</b>, Yao Xie. 2022 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9518091/">Small-Sample Inferred Adaptive Recoding for Batched Network Coding</a><br />
<b>Jie Wang</b>, Zhiyuan Jia, Hoover H. F. Yin, Shenghao Yang. 2021 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
<li><p><a href="Final_paper.pdf">Reliable Offline Pricing in eCommerce Decision-Making: A Distributionally Robust Viewpoint</a><br /> 
<b>Jie Wang</b>. Finalist presentation for the 18th INFORMS DMDA Workshop Data Challenge Competition, 2024</p>
</li>
</ol>
<h3>Efficient Statistical Hypothesis Testing</h3>
<p>Hypothesis testing has long been a challenge in statistics, involving the decision to accept or reject a null hypothesis based on collected observations. 
However, classical methods often fall short in addressing the challenges posed by the era of Big Data. 
In my research, I have developed modern hypothesis testing approaches to tackle these difficulties.</p>
<p>A key focus of my work has been developing efficient hypothesis testing frameworks for high-dimensional data. 
Traditional methods often suffer significant performance degradation as data dimensionality increases. 
To address this issue, we utilize nonlinear dimensionality reduction to project data distributions onto low-dimensional spaces with maximum separability 
before conducting hypothesis tests.</p>
<p>Another area of my research bridges hypothesis testing with deep learning, providing a statistical foundation for reliable machine learning. 
Our goal is to create systematic tools that offer statistical performance guarantees for hypothesis testing using neural networks. 
This advancement aims to make modern classification algorithms more dependable and trustworthy, particularly in scientific discovery.</p>
<p>Additionally, I have explored the synergy between hypothesis testing and recent advances in optimization to develop efficient testing methodologies for various scenarios. 
For example, I employ distributionally robust optimization to create a non-parametric test, assuming that data distributions under each hypothesis belong to &ldquo;uncertainty sets&rdquo; constructed using the Sinkhorn discrepancy. 
Furthermore, I have investigated variable selection for hypothesis testing, aiming to identify a small subset of variables that best distinguish samples from different groups. 
Due to the inherent sparsity, this problem is often formulated as a challenging NP-hard mixed-integer programming task. 
Both exact and approximation algorithms are provided to address the computational challenges,</p>
<table class="imgtable"><tr><td>
<img src="fig_b1.png" alt="alt text" height="260px" />&nbsp;</td>
<td align="left"></td></tr></table>
<p>Below I highlight manuscripts related to this topic:</p>
<ol>
<li><p><a href="https://arxiv.org/abs/2405.15441">Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances</a><br />
<b>Jie Wang</b>, March Boedihardjo, Yao Xie. (<font color=red <b>Finalist of the INFORMS 2024 Data Mining Best Paper Award Competition</b></font>)</p>
</li>
<li><p><a href="https://arxiv.org/abs/2302.07415">Variable Selection for Kernel Two-Sample Tests</a><br />
<b>Jie Wang</b>, Santanu Dey, Yao Xie. (<font color=red <b>Selected for Poster Presentation at Mixed Integer Programming (MIP) Workshop 2023</b></font>, to be submitted to <a href="https://pubsonline.informs.org/journal/opre">Operations Research</a>)<br /></p>
</li>
<li><p><a href="https://doi.org/10.1093/imaiai/iaad018">A Manifold Two-Sample Test Study: Integral Probability Metric with Neural Networks</a><br />
<b>Jie Wang</b>, Minshuo Chen, Tuo Zhao, Wenjing Liao, Yao Xie. <a href="https://academic.oup.com/imaiai/article/12/3/1867/7195202">Information and Inference: A Journal of the IMA</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2403.14822">Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets</a><br />
<b>Jie Wang</b>, Rui Gao, Yao Xie. <a href="https://2024.ieee-isit.org/home">2024 IEEE International Symposium on Information Theory</a></p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9834367">A Data-Driven Approach to Robust Hypothesis Testing Using Sinkhorn Uncertainty Sets</a><br />
<b>Jie Wang</b>, Yao Xie. 2022 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
<li><p><a href="https://proceedings.mlr.press/v151/wang22f.html">Two-sample Test with Kernel Projected Wasserstein Distance</a><br />
<b>Jie Wang</b>, Rui Gao, Yao Xie. 2022 Artificial Intelligence and Statistics (AISTATS) (<font color=red <b>Oral Presentation! Rate: 44/1685=0.026</b></font>)</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9518186">Two-sample Test using Projected Wasserstein Distance</a><br />
<b>Jie Wang</b>, Rui Gao, Yao Xie. 2021 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
</ol>
<h3>Reliable Multi-Hop Network Communication</h3>
<p>Over the past decade, wireless network communication products such as WiFi and cellular networks have become widely accessible across the globe. 
However, a common challenge in the industry, known as the &ldquo;curse of multihop,&rdquo; involves a significant decrease in network throughput as the number of transmission hops increases. 
To address this issue, a recently developed technique called batched network coding offers a computationally efficient solution. 
My research has focused on two key aspects of this technique.</p>
<ul>
<li><p>First, I conducted theoretical analyses to determine the scaling rate of batched network coding concerning hop length, 
demonstrating its advantages over traditional methods like decode-and-forward. </p>
</li>
<li><p>Second, I developed an enhanced batched network coding methodology that incorporates considerations for the uncertainty in channel status, 
further optimizing network performance.</p>
</li>
</ul>
<table class="imgtable"><tr><td>
<img src="fig_a.png" alt="alt text" height="260px" />&nbsp;</td>
<td align="left"></td></tr></table>
<p>Below I highlight manuscripts related to this topic:</p>
<ol>
<li><p><a href="https://ieeexplore.ieee.org/document/10445298">On Achievable Rates of Line Networks with Generalized Batched Network Coding</a><br />
<b>Jie Wang</b>, Shenghao Yang, Yanyan Dong, Yiheng Zhang. <a href="https://www.comsoc.org/publications/journals/ieee-jsac/cfp/space-communications-new-frontiers-near-earth-deep-space">IEEE Journal on Selected Areas in Communications</a></p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/10571545">Throughput and Latency Analysis for Line Networks with Outage Links</a><br />
Yanyan Dong, Shenghao Yang, <b>Jie Wang</b>, Fan Cheng. <a href="https://www.itsoc.org/jsait">IEEE Journal on Selected Areas in Information Theory</a> (Conference version accepted by <a href="https://2024.ieee-isit.org/home">2024 IEEE International Symposium on Information Theory</a>)</p>
</li>
<li><p><a href="">Sparse Degree Optimization for BATS Codes</a><br />
Hoover H. F. Yin, <b>Jie Wang</b>. <a href="http://www.ieee-itw2024.org/">2024 IEEE Information Theory Workshop</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2405.08194">Distributionally Robust Degree Optimization for BATS Codes</a><br />
Hoover H. F. Yin, <b>Jie Wang</b>, Sherman S. M. Chow. <a href="https://2024.ieee-isit.org/home">2024 IEEE International Symposium on Information Theory</a></p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/10476945">Reliable Adaptive Recoding for Batched Network Coding with Burst-Noise Channels</a><br />
<b>Jie Wang</b>, Talha Bozkus, Yao Xie, Urbashi Mitra. Asilomar 2023</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/document/9518091/">Small-Sample Inferred Adaptive Recoding for Batched Network Coding</a><br />
<b>Jie Wang</b>, Zhiyuan Jia, Hoover H. F. Yin, Shenghao Yang. 2021 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/9174441/">Upper Bound Scalability on Achievable Rates of Batched Codes for Line Networks</a><br />
Shenghao Yang, <b>Jie Wang</b>. 2020 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
<li><p><a href="https://ieeexplore.ieee.org/abstract/document/8849792/">On the Capacity Scalability of Line Networks with Buffer Size Constraints</a><br />
Shenghao Yang, <b>Jie Wang</b>, Yanyan Dong, Yiheng Zhang. 2019 IEEE International Symposium on Information Theory (ISIT)</p>
</li>
<li><p><a href="IT_newsletter.pdf">Finite-length Code and Application in Network Coding</a><br />
Shenghao Yang, Yanyan Dong, <b>Jie Wang</b>. IEEE INFORMATION THEORY SOCIETY GUANGZHOU CHAPTER NEWSLETTER, No.1, July 2020</p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
Page generated 2024-08-28 17:37:58 +08, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
