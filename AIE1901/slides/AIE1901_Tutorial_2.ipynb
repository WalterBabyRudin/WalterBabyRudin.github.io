{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "K_E7oYCl9s3q"
      },
      "outputs": [],
      "source": [
        "# Use a leading exclamation mark ! to change the code cell to treating the input as a shell script\n",
        "! pip install datasets transformers\n",
        "! pip install vllm\n",
        "! pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset\n",
        "### check the OR-Instruct-Data-3K dataset at https://huggingface.co/datasets/CardinalOperations/OR-Instruct-Data-3K"
      ],
      "metadata": {
        "id": "w2NWE04C-azS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"CardinalOperations/OR-Instruct-Data-3K\")"
      ],
      "metadata": {
        "id": "adGrZrFc-ZDZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Meet your dataset\n",
        "This dataset consists of 2 columns:\n",
        "1. Prompt (string): consists of system instructions\n",
        "2. Completion (string): Consists of responses/answers to system instructors"
      ],
      "metadata": {
        "id": "cNSeKgek2hGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "pBUjxL_I-0w1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0] # Accessing the first record"
      ],
      "metadata": {
        "id": "YGLH5XoO-5Ij",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To deal with the case where the recod is difficult to read"
      ],
      "metadata": {
        "id": "514tiyqC25PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_template(data_dict):\n",
        "    \"\"\"\n",
        "    Convert a dictionary with 'prompt' and 'completion' keys to the specified template format.\n",
        "\n",
        "    Args:\n",
        "        data_dict (dict): Dictionary containing 'prompt' and 'completion' keys\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string in the template format\n",
        "    \"\"\"\n",
        "    template = r\"\"\"Below is an operations research question. Build a mathematical model and corresponding python code using `coptpy` that appropriately addresses the question.\n",
        "\n",
        "# Question:\n",
        "{Question}\n",
        "\n",
        "# Response:\n",
        "{Response}\"\"\"\n",
        "    # Extract the question from the prompt (removing the fixed prefix)\n",
        "    prompt_text = data_dict['prompt']\n",
        "    question_start = prompt_text.find('# Question:') + len('# Question:')\n",
        "    question_text = prompt_text[question_start:].strip()\n",
        "    # Get the completion/response\n",
        "    response_text = data_dict['completion']\n",
        "    # Format using the template\n",
        "    formatted_output = template.format(Question=question_text, Response=response_text)\n",
        "    return formatted_output\n",
        "\n",
        "def save_string_to_file(content, filename, mode='w', encoding='utf-8'):\n",
        "    \"\"\"\n",
        "    Save a string to a text file.\n",
        "\n",
        "    Args:\n",
        "        content (str): The string content to save\n",
        "        filename (str): The name/path of the file to create\n",
        "        mode (str): File mode - 'w' for write (overwrite), 'a' for append\n",
        "        encoding (str): File encoding (default: 'utf-8')\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filename, mode, encoding=encoding) as file:\n",
        "            file.write(content)\n",
        "        print(f\"Successfully saved content to {filename}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "        return False\n",
        "\n",
        "result = convert_to_template(dataset['train'][0])\n",
        "save_string_to_file(result, \"result.txt\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "9H7d94wLErYu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling copt solver to solve this optimizaiton problem"
      ],
      "metadata": {
        "id": "Us_Gw2Wj3gEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install coptpy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xleUKP_O3SnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import coptpy as cp\n",
        "from coptpy import COPT\n",
        "\n",
        "# Create a COPT environment\n",
        "env = cp.Envr()\n",
        "\n",
        "# Create a model\n",
        "model = env.createModel(\"GeothermalPowerPlantScheduling\")\n",
        "\n",
        "# Define decision variables\n",
        "x_A = model.addVar(lb=0, ub=1000, name=\"x_A\")  # Extraction quantity of well A\n",
        "x_B = model.addVar(lb=0, ub=1500, name=\"x_B\")  # Extraction quantity of well B\n",
        "x_C = model.addVar(lb=0, ub=2000, name=\"x_C\")  # Extraction quantity of well C\n",
        "\n",
        "# Define the objective function\n",
        "model.setObjective(x_A + x_B + x_C - 5*x_A - 4*x_B - 3*x_C - 2*(x_A + x_B + x_C), sense=COPT.MAXIMIZE)\n",
        "\n",
        "# Add constraints\n",
        "model.addConstr(x_A + x_B + x_C == 2800, name=\"MarketDemand\")  # Electricity market demand constraint\n",
        "model.addConstr(x_A + x_B + x_C <= 3000, name=\"EquipmentCapacity\")  # Equipment operating capacity constraint\n",
        "model.addConstr(0.4*(x_A + x_B + x_C) <= x_A + x_B + x_C, name=\"ReinjectionRatio\")  # Reinjection ratio constraint\n",
        "\n",
        "# Solve the model\n",
        "model.solve()\n",
        "\n",
        "# Output the results\n",
        "if model.status == COPT.OPTIMAL:\n",
        "    print(\"Maximum total revenue: {:.2f}\".format(model.objval))\n",
        "    print(\"Scheduling plan:\")\n",
        "    print(f\"Extraction quantity of well A: {x_A.x}\")\n",
        "    print(f\"Extraction quantity of well B: {x_B.x}\")\n",
        "    print(f\"Extraction quantity of well C: {x_C.x}\")\n",
        "else:\n",
        "    print(\"No optimal solution found.\")"
      ],
      "metadata": {
        "id": "EZx4_Xnlvhj5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the feature structure of the training set\n",
        "dataset['train'].features"
      ],
      "metadata": {
        "id": "r6w3nmOE-9Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of rows/examples in the training dataset\n",
        "dataset['train'].num_rows"
      ],
      "metadata": {
        "id": "j4g_hMTm-_EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first prompt in the training dataset\n",
        "dataset[\"train\"][\"completion\"][0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tMKuzXhf3sGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"prompt\"][0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vDZHEOzd6c8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modify your dataset"
      ],
      "metadata": {
        "id": "qV4q_Hf44Aln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Randomly shuffle all rows in the training dataset while setting a random seed for reproducible results\n",
        "shuffled_dataset = dataset[\"train\"].shuffle(seed=42)\n",
        "# Shuffle: to prevent learning order-based patterns\n",
        "\n",
        "# Create a subset of the training dataset by selecting the first 5 rows using index range [0-4]\n",
        "selected_dataset = dataset[\"train\"].select(range(5))\n",
        "\n",
        "# Split the training dataset into train and validation sets\n",
        "# 80% of data goes to train_dataset\n",
        "# 20% of data goes to valid_dataset (test_size=0.2)\n",
        "# seed=42 ensures reproducible splitting\n",
        "train_dataset, valid_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Split dataset into multiple smaller parts (shards) for distributed processing\n",
        "# - num_shards=5: Divide dataset into 5 equal parts\n",
        "# - index=0: Select the first shard (indices 0, 5, 10, ...)\n",
        "sharded_dataset = dataset[\"train\"].shard(num_shards=5, index=0)\n",
        "print(\"\\nSharded dataset size:\", len(sharded_dataset))\n",
        "print(\"Original dataset size:\", len(dataset[\"train\"]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IvuRCHR_4B1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Exporting Data"
      ],
      "metadata": {
        "id": "BGuRSJmI4ONd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.save_to_disk(\"./\")"
      ],
      "metadata": {
        "id": "Mzes67X14P12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_csv(\"./dataset.csv\")\n",
        "dataset[\"train\"].to_json(\"./dataset.json\")\n",
        "dataset[\"train\"].to_parquet(\"./dataset.parquet\")"
      ],
      "metadata": {
        "id": "I6C6RZRj4X-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load a pretrained model and chat with it"
      ],
      "metadata": {
        "id": "siwx6hhl43CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "model_id = \"CardinalOperations/ORLM-LLaMA-3-8B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")"
      ],
      "metadata": {
        "id": "Rxqk57U3KZv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=8000)\n",
        "\n",
        "def my_convert_to_template(data_dict):\n",
        "    \"\"\"\n",
        "    Convert prompt text to the specified template format.\n",
        "\n",
        "    Args:\n",
        "        data_dict (dict): Dictionary containing 'generated_text' key\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string in the template format\n",
        "    \"\"\"\n",
        "    template = r\"\"\"Below is an operations research question. Build a mathematical model and corresponding python code using `coptpy` that appropriately addresses the question.\n",
        "\n",
        "# Question:\n",
        "{Question}\n",
        "\n",
        "# Response:\n",
        "{Response}\"\"\"\n",
        "\n",
        "    # Extract the question from the prompt (removing the fixed prefix)\n",
        "    prompt_text = data_dict['generated_text']\n",
        "    question_start = prompt_text.find('# Question:')\n",
        "    response_start = prompt_text.find(\"# Response:\", question_start)# - len('# Response:')\n",
        "\n",
        "    question_text = prompt_text[question_start + len('# Question:'):response_start].strip()\n",
        "    response_text = prompt_text[response_start + len('# Response:'):].strip()\n",
        "    # Format using the template\n",
        "    formatted_output = template.format(Question=question_text, Response=response_text)\n",
        "    return formatted_output\n",
        "\n",
        "question = dataset[\"train\"][\"prompt\"][0]\n",
        "print(question)\n",
        "answer = pipe(question)\n",
        "print(answer)\n",
        "formatted_answer = my_convert_to_template(answer[0])\n",
        "print(formatted_answer)\n"
      ],
      "metadata": {
        "id": "GTBV2CuLMYlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}